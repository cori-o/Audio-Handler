from pydub.effects import high_pass_filter, low_pass_filter
from nsnet2_denoiser import NSnet2Enhancer
from demucs.pretrained import get_model
from demucs.apply import apply_model
from pydub import AudioSegment
import matplotlib.pyplot as plt
import pyloudnorm as pyln
import noisereduce as nr
import soundfile as sf
import numpy as np
import subprocess
import tempfile
import torchaudio
import librosa
import torch
import io
import os 


class NoiseHandler: 
    '''
    ìŒì„± íŒŒì¼ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•œë‹¤.
    '''
    def remove_background_noise(self, input_file, output_file=None, prop_decrease=None):
        """
        ë°°ê²½ ì¡ìŒì„ ì œê±°í•˜ì—¬ ê°€ê¹Œìš´ ëª©ì†Œë¦¬ë¥¼ ê°•ì¡°
        args:
            input_file (str): ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼.
            output_file (str): ì¶œë ¥ ì˜¤ë””ì˜¤ íŒŒì¼.
        """
        try:
            y, sr = librosa.load(input_file, sr=None)   # ì˜¤ë””ì˜¤ ë¡œë“œ
        except:
            audio_buffer = io.BytesIO()
            input_file.export(audio_buffer, format="wav")
            audio_buffer.seek(0)  # ë²„í¼ì˜ ì‹œì‘ ìœ„ì¹˜ë¡œ ì´ë™
            y, sr = librosa.load(audio_buffer, sr=None)
        # noise_profile = y[:sr]   # ë°°ê²½ ì¡ìŒ í”„ë¡œíŒŒì¼ ì¶”ì¶œ (ì´ˆê¸° 1ì´ˆ)
        # y_denoised = nr.reduce_noise(y=y, sr=sr, y_noise=noise_profile, prop_decreaese=prop_decrease)   # ì¡ìŒ ì œê±°
        y_denoised = nr.reduce_noise(y=y, sr=sr, prop_decrease=prop_decrease)
        if output_file: 
            sf.write(output_file, y_denoised, sr)
            print(f"Saved denoised audio to {output_file}")
        
        wav_buffer = io.BytesIO()   # ë©”ëª¨ë¦¬ ë‚´ WAV íŒŒì¼ ìƒì„±
        sf.write(wav_buffer, y_denoised, sr, format='WAV')
        wav_buffer.seek(0)   # íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ì´ë™
        return wav_buffer

    def filter_audio_with_ffmpeg(self, input_file, high_cutoff=100, low_cutoff=3500, output_file=None):
        """
        FFmpegì„ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ í•„í„°ë§ (ê³ ì—­ëŒ€, ì €ì—­ëŒ€).
        Args:
            input_file (str or BytesIO): ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” BytesIO ê°ì²´.
            high_cutoff (int): ê³ ì—­ í•„í„° ì»·ì˜¤í”„ ì£¼íŒŒìˆ˜ (Hz).
            low_cutoff (int): ì €ì—­ í•„í„° ì»·ì˜¤í”„ ì£¼íŒŒìˆ˜ (Hz).
            output_file (str, optional): í•„í„°ë§ëœ ì˜¤ë””ì˜¤ ì €ì¥ ê²½ë¡œ. ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ë©”ëª¨ë¦¬ë¡œ ë°˜í™˜.
        Returns:
            io.BytesIO: í•„í„°ë§ëœ ì˜¤ë””ì˜¤ ë°ì´í„° (output_fileì´ Noneì¸ ê²½ìš°).
        """
        input_source = None   # ë³€ìˆ˜ ì´ˆê¸°í™”
        temp_files = []   # ì„ì‹œ íŒŒì¼ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        try:
            if isinstance(input_file, AudioSegment):   # AudioSegment ê°ì²´ ì²˜ë¦¬
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_input:
                    input_file.export(temp_input, format="wav")   # AudioSegment -> WAV ë³€í™˜
                    temp_input.flush()
                    input_source = temp_input.name
                    temp_files.append(temp_input.name)   # ì„ì‹œ íŒŒì¼ ê´€ë¦¬
            elif isinstance(input_file, io.BytesIO):   # BytesIO ê°ì²´ ì²˜ë¦¬
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_input:
                    temp_input.write(input_file.getvalue())
                    temp_input.flush()
                    input_source = temp_input.name
                    temp_files.append(temp_input.name)   # ì„ì‹œ íŒŒì¼ ê´€ë¦¬
            elif isinstance(input_file, (str, os.PathLike)):   # íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
                input_source = input_file
            else:
                raise ValueError("Invalid input_file type. Must be AudioSegment, file path, or BytesIO object.")

            if input_source is None:
                raise RuntimeError("Failed to determine input source.")

            command = [   # FFmpeg ëª…ë ¹ ì‹¤í–‰
                "ffmpeg",
                "-i", input_source,  # ì…ë ¥ íŒŒì¼
                "-af", f"highpass=f={high_cutoff},lowpass=f={low_cutoff}",  # í•„í„° ì ìš©
                "-f", "wav",  # ì¶œë ¥ í˜•ì‹
                "pipe:1" if not output_file else output_file  # ë©”ëª¨ë¦¬ë¡œ ë°˜í™˜í•˜ê±°ë‚˜ íŒŒì¼ë¡œ ì €ì¥
            ]
            if output_file:
                subprocess.run(command, check=True)
                print(f"Filtered audio saved to {output_file}")
            else:
                process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                stdout, stderr = process.communicate()
                if process.returncode != 0:
                    raise RuntimeError(f"FFmpeg error: {stderr.decode()}")
                return io.BytesIO(stdout)  # BytesIOë¡œ ë°˜í™˜
        finally:
            for temp_file in temp_files:   # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if os.path.exists(temp_file):
                    os.unlink(temp_file)

    def separate_vocals_with_demucs(self, audio_file, output_dir='dataset/demucs'):
        env = os.environ.copy()
        env["MKL_THREADING_LAYER"] = "GNU"
        try:
            subprocess.run([
                "demucs",
                "--two-stems", "vocals",
                "--out", output_dir,
                audio_file
            ], check=True, env=env)
            print(f"Separated vocals saved in {output_dir}")
        except subprocess.CalledProcessError as e:
            print("ğŸš¨ Demucs ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)

    def denoise_audio(self, audio_file, model_type='nsnet', data_path=None):
        if model_type == 'nsnet':
            enhancer = NSnet2Enhancer(fs=48000)
            sigIn, fs = sf.read(audio_file)
            outSig = enhancer(sigIn, fs)

            audioIn = AudioSegment.from_wav(audio_file)
            audioOut = enhancer.pcm_16le(audioIn.raw_data)
            audio_clean = AudioSegment(
                data=audioOut,
                sample_width=2,         # 16-bit PCM = 2 bytes
                frame_rate=audioIn.frame_rate,
                channels=audioIn.channels
            )
            if data_path: 
                file_name = 'denoised_' + audio_file.split('/')[-1].split('.')[0] + '.wav'
                save_path = os.path.join(data_path, file_name)
                audio_clean.export(save_path, format="wav")
                print(f"âœ”ï¸ ì¡ìŒ ì œê±°ëœ ì˜¤ë””ì˜¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")
        elif model_type == '':
            pass



class VoiceEnhancer:
    '''
    ìŒì„± íŒŒì¼ì—ì„œ ìŒì„±ì„ ê°•í™”í•œë‹¤. 
    '''
    def emphasize_nearby_voice(self, input_file, threshold=0.05, output_file=None):
        """
        ê°€ê¹Œìš´ ìŒì„±ì„ ê°•ì¡°í•˜ê³  ë¨¼ ëª©ì†Œë¦¬ë¥¼ ì¤„ì„
        args:
            input_file (str): ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼
            output_file (str): ì¶œë ¥ ì˜¤ë””ì˜¤ íŒŒì¼
            threshold (float): ì—ë„ˆì§€ ê¸°ì¤€ê°’ (ë‚®ì„ìˆ˜ë¡ ì•½í•œ ì‹ í˜¸ ì œê±°)
        """
        try:
            y, sr = librosa.load(input_file, sr=None)   # ì˜¤ë””ì˜¤ ë¡œë“œ
        except:
            audio_buffer = io.BytesIO()
            input_file.export(audio_buffer, format="wav")
            audio_buffer.seek(0)  # ë²„í¼ì˜ ì‹œì‘ ìœ„ì¹˜ë¡œ ì´ë™
            y, sr = librosa.load(audio_buffer, sr=None)           
        rms = librosa.feature.rms(y=y)[0]         # RMS ì—ë„ˆì§€ ê³„ì‚°
        mask = rms > threshold                    # ì—ë„ˆì§€ ê¸°ì¤€ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„±

        expanded_mask = np.repeat(mask, len(y) // len(mask) + 1)[:len(y)]   # RMS ê°’ì„ ì „ì²´ ì‹ í˜¸ ê¸¸ì´ì— ë§ê²Œ í™•ì¥
        y_filtered = y * expanded_mask.astype(float)   # ì…ë ¥ ì‹ í˜¸ì— í™•ì¥ëœ ë§ˆìŠ¤í¬ ì ìš©

        if output_file:
            sf.write(output_file, y_filtered, sr)   # ê°•ì¡°ëœ ì˜¤ë””ì˜¤ ì €ì¥
            print(f"Saved emphasized audio to {output_file}")
        else:
            audio_buffer = io.BytesIO()
            sf.write(audio_buffer, y_filtered, sr, format="WAV")
            audio_buffer.seek(0)  # ë²„í¼ì˜ ì‹œì‘ ìœ„ì¹˜ë¡œ ì´ë™
            return audio_buffer

    def normalize_audio_lufs(self, audio_input, target_lufs=-14.0, output_file=None):
        """
        LUFS ê¸°ë°˜ ì˜¤ë””ì˜¤ ì •ê·œí™”
        """
        #print(audio_input)
        if isinstance(audio_input, io.BytesIO):
            audio_input.seek(0)
            data, rate = sf.read(audio_input)
        else:
            data, rate = sf.read(audio_input)

        # í˜„ì¬ LUFS ê³„ì‚° ë° ì •ê·œí™”
        meter = pyln.Meter(rate)
        loudness = meter.integrated_loudness(data)
        loudness_normalized_audio = pyln.normalize.loudness(data, loudness, target_lufs)

        if output_file:
            sf.write(output_file, loudness_normalized_audio, rate)
            print(f"Saved normalized audio to {output_file}")
            return output_file
        else:
            wav_buffer = io.BytesIO()
            sf.write(wav_buffer, loudness_normalized_audio, rate, format='WAV')
            wav_buffer.seek(0)
            return wav_buffer


class AudioVisualizer:
    def __init__(self, n_fft=1024, hop_length=256, n_mels=128):
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels

    def get_waveform(self, y, sr, title_prefix="", file_name=None):
        plt.figure(figsize=(14, 3))
        librosa.display.waveshow(y, sr=sr)
        plt.title(f"{title_prefix} - Waveform")
        plt.tight_layout()
        if file_name:
            plt.savefig(file_name, dpi=300)
        plt.close()

    def get_spectrogram(self, y, sr, title_prefix="", file_name=None):
        D = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=self.n_fft, hop_length=self.hop_length)), ref=np.max)
        plt.figure(figsize=(14, 3))
        librosa.display.specshow(D, sr=sr, hop_length=self.hop_length, x_axis='time', y_axis='hz', cmap='magma')
        plt.colorbar(format="%+2.0f dB")
        plt.title(f"{title_prefix} - STFT Spectrogram")
        plt.tight_layout()
        if file_name:
            plt.savefig(file_name, dpi=300)
        plt.close()

    def get_melspectrogram(self, y, sr, title_prefix="", file_name=None):
        S = librosa.feature.melspectrogram(
            y=y, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length, n_mels=self.n_mels, power=2.0
        )
        S_dB = librosa.power_to_db(S, ref=np.max)
        plt.figure(figsize=(14, 3))
        librosa.display.specshow(S_dB, sr=sr, hop_length=self.hop_length, x_axis='time', y_axis='mel', cmap='magma')
        plt.colorbar(format="%+2.0f dB")
        plt.title(f"{title_prefix} - Mel Spectrogram")
        plt.tight_layout()
        if file_name:
            plt.savefig(file_name, dpi=300)
        plt.close()

    def visualize_all(self, y, sr, title_prefix="", file_name=None):
        # ì „ì²´ë¥¼ í•œ ë²ˆì— ì‹œê°í™” (waveform + STFT + Mel)
        S = librosa.feature.melspectrogram(
            y=y, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length, n_mels=self.n_mels, power=2.0
        )
        S_dB = librosa.power_to_db(S, ref=np.max)
        D = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=self.n_fft, hop_length=self.hop_length)), ref=np.max)

        plt.figure(figsize=(14, 10))

        # 1. Waveform
        plt.subplot(3, 1, 1)
        librosa.display.waveshow(y, sr=sr)
        plt.title(f"{title_prefix} - Waveform")

        # 2. STFT Spectrogram
        plt.subplot(3, 1, 2)
        librosa.display.specshow(D, sr=sr, hop_length=self.hop_length, x_axis='time', y_axis='hz', cmap='magma')
        plt.colorbar(format="%+2.0f dB")
        plt.title(f"{title_prefix} - STFT Spectrogram")

        # 3. Mel Spectrogram
        plt.subplot(3, 1, 3)
        librosa.display.specshow(S_dB, sr=sr, hop_length=self.hop_length, x_axis='time', y_axis='mel', cmap='magma')
        plt.colorbar(format="%+2.0f dB")
        plt.title(f"{title_prefix} - Mel Spectrogram")

        plt.tight_layout()
        if file_name:
            plt.savefig(file_name, dpi=300)
        plt.close()

    def visualize_before_after_all(self, y_before, sr_before, y_after, sr_after, file_name=None):
        n_fft = 1024
        hop_length = 256
        n_mels = 128

        S_before = librosa.feature.melspectrogram(y=y_before, sr=sr_before, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=2.0)
        S_dB_before = librosa.power_to_db(S_before, ref=np.max)
        D_before = librosa.amplitude_to_db(np.abs(librosa.stft(y_before, n_fft=n_fft, hop_length=hop_length)), ref=np.max)

        S_after = librosa.feature.melspectrogram(y=y_after, sr=sr_after, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=2.0)
        S_dB_after = librosa.power_to_db(S_after, ref=np.max)
        D_after = librosa.amplitude_to_db(np.abs(librosa.stft(y_after, n_fft=n_fft, hop_length=hop_length)), ref=np.max)

        # --- Plot ìˆœì„œ ---
        plt.figure(figsize=(18, 28))
        
        # 1. Waveform
        plt.subplot(6, 1, 1)
        librosa.display.waveshow(y_before, sr=sr_before)
        plt.title("Before - Waveform", fontsize=14)

        plt.subplot(6, 1, 2)
        librosa.display.waveshow(y_after, sr=sr_after)
        plt.title("After - Waveform", fontsize=14)

        # 2. STFT
        plt.subplot(6, 1, 3)
        librosa.display.specshow(D_before, sr=sr_before, hop_length=hop_length, x_axis='time', y_axis='hz', cmap='magma')
        plt.colorbar(format="%+2.0f dB")
        plt.title("Before - STFT Spectrogram", fontsize=14)

        plt.subplot(6, 1, 4)
        librosa.display.specshow(D_after, sr=sr_after, hop_length=hop_length, x_axis='time', y_axis='hz', cmap='magma')
        plt.colorbar(format="%+2.0f dB")
        plt.title("After - STFT Spectrogram", fontsize=14)

        # 3. Mel
        plt.subplot(6, 1, 5)
        librosa.display.specshow(S_dB_before, sr=sr_before, hop_length=hop_length, x_axis='time', y_axis='mel', cmap='magma')
        plt.colorbar(format="%+2.0f dB")
        plt.title("Before - Mel Spectrogram", fontsize=14)

        plt.subplot(6, 1, 6)
        librosa.display.specshow(S_dB_after, sr=sr_after, hop_length=hop_length, x_axis='time', y_axis='mel', cmap='magma')
        plt.colorbar(format="%+2.0f dB")
        plt.title("After - Mel Spectrogram", fontsize=14)
        plt.subplots_adjust(hspace=0.8)
        plt.tight_layout(pad=3)
        if file_name:
            plt.savefig(file_name, dpi=300, bbox_inches='tight')
        plt.close()